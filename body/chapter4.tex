\section{从拓扑不变量和时间序列中提取有效特征} % <--- 这是本章唯一的 \section
\label{chap:feature_extraction}


    \subsection{基于持续同调的特征构造方法}
        \label{sec:tda_features}
        % 核心：系统介绍将持久性图（PD）或其他PH输出转化为特征向量的方法。
        在现代数据分析领域，面对日益增长的数据复杂性，例如三维点云数据和时间序列数据，开发能够捕捉数据内在结构和本质信息的特征构造方法至关重要。传统的特征提取方法往往侧重于数据的局部几何属性或统计特性，但在揭示高维数据的全局结构和拓扑形态方面能力有限。拓扑数据分析（Topological Data Analysis, TDA）作为近年来新兴的强大工具，特别是其核心方法——持续同调（Persistent Homology, PH），为从复杂数据集中提取鲁棒的拓扑特征提供了一个全新的视角 (例如，参考文献 )。

持续同调源于代数拓扑学，其核心思想是通过分析数据在不同尺度下的拓扑结构演化，来识别那些“持续”存在的、能够反映数据本质形态的特征，如连通分支（0维孔洞）、环状结构（1维孔洞）、空腔（2维孔洞）等 (参考文献)。与传统方法不同，持续同调能够捕捉数据的多尺度全局信息，并且对噪声和度量空间的微小扰动具有一定的稳定性 (参考文献 )。这些优点使得基于持续同调的特征在形状分类、分割、降噪、时间序列分析、点云理解、金融风险预测、生物医学以及材料科学等众多领域展现出巨大的应用潜力 (例如，参考文献 )。

持续同调的计算结果通常被总结在持续图（Persistence Diagram, PD）中 (参考文献 )。持续图是一个信息丰富的多尺度描述符，它将数据中拓扑特征的“出生”（birth）和“死亡”（death）时间表示为二维平面上的一个多重点集。图上每个点 $(b_i, d_i)$ 对应一个拓扑特征，其横坐标 $b_i$ 是该特征出现的尺度，纵坐标 $d_i$ 是其消失的尺度，而 $d_i - b_i$ 则被称为该特征的“持久性”或“寿命”（persistence/lifetime）。

然而，持续图本身作为一个数学对象（多重点集），其结构复杂且度量空间不完备，这使得它无法直接输入到标准的机器学习算法或深度学习框架中进行分析 (参考文献)。因此，核心的挑战在于如何有效地量化和总结持续图中蕴含的拓扑信息，将其转化为具体的、可计算的、适用于下游任务的特征表示（Feature Representation）或描述符（Descriptor）。这正是持续同调特征构造方法研究的关键所在 (参考文献 )。

为了应对这一挑战，研究者们发展了多种策略来从持续图中提取特征。这些方法大致可以归纳为以下几类，也构成了本章将要深入探讨的主要内容。一种直接的方法是计算持久性图的统计描述符（Statistical Descriptors of PDs）。这类方法通过计算持续图上点的坐标（出生时间、死亡时间、持久性）的各种统计量来概括其分布特性。例如，可以计算不同维度同调群的特征总数、所有特征的总寿命或平均寿命（参考文献  中讨论了类似指标）、最长寿命（反映最显著特征，参考文献 ）、显著特征（寿命超过某个阈值）的个数（参考文献 ），以及基于寿命分布计算的熵值等 (参考文献 )。这些统计量提供了对持续图整体信息的宏观概括。

另一类方法侧重于基于贝蒂数（Betti Numbers）的表示。这类方法关注在不同尺度下拓扑特征的数量。贝蒂数 $\beta_k$ 直接量化了 $k$ 维孔洞的数量。通过追踪贝蒂数随尺度参数的变化，可以得到如贝蒂曲线（Betti Curves）等表示方法（参考文献 ），它们反映了数据在不同尺度下的拓扑复杂度。

此外，还有一系列方法旨在将整个持续图的结构映射到一个更适合机器学习处理的空间，例如向量空间或函数空间。这些向量或函数空间嵌入方法中，有几种具有代表性。持续同调熵（Persistence Entropy, PE）是一种基于熵的标量描述符，形式简洁但可能信息损失较大（参考文献）。持续性图像（Persistence Images, PI）通过对持续图上的点应用加权核函数（如高斯核）并进行积分，将其转化为稳定且维度固定的图像或向量表示（参考文献 ）。持续性景观（Persistence Landscapes, PL）将持续图上的每个点转换为一个“帐篷”函数，并取这些函数的 $k$ 次最大值，得到一系列分段线性函数，构成函数空间中的一个元素（参考文献 ）。持久性中心（Persistence Centers, PC）则将持续图上的点视为带权重的点（权重通常与其持久性相关），计算这些点的加权平均中心，得到一个（或多个）代表性的点向量（参考文献 ）。

理解这些不同的特征构造方法的原理、计算方式、特性（如稳定性、可解释性、计算复杂度）及其优缺点，对于在具体应用中选择和设计合适的拓扑特征至关重要。本章旨在对上述几类基于持续同调的特征构造方法——即持久性图的统计描述符、基于贝蒂数的表示，以及具体的矢量化技术 PE、PI、PL 和 PC——进行详细的介绍和分析。通过对这些方法的阐述，我们将为读者构建一个关于如何从持续同调计算结果中提取有效特征的知识框架，为后续的理论研究和实际应用奠定基础。

        \subsubsection{持久性图的统计描述符}
            \label{sec:pd_stats}
            % 原理：计算PD点的全局或维度相关的统计量。
            % 示例：点的数量、寿命统计（总、均、最值、方差）、持久性熵、基于坐标/寿命的范数等。
            % 文献：引用 Karan & Kaygun (2021) 中使用的熵和距离相关统计量。
            % 优缺点：简单直观，但信息损失大，对噪声可能敏感。
            在将持续同调应用于数据分析时，我们获得了信息丰富的持久性图（Persistence Diagram, PD）。然而，PD本身作为一个在$\mathbb{R}^2$空间中的多重点集 $\{(b_i, d_i)\}$，其结构不便于直接整合到许多标准的统计或机器学习框架中。为了克服这一障碍，研究者们提出了一系列方法来量化和总结PD中包含的拓扑信息，其中一类直接且直观的方法是计算PD的统计描述符。这些描述符旨在通过若干数值指标来捕捉PD的整体或关键特性，从而将抽象的拓扑结构信息转化为可操作的定量特征。

            这类统计描述符通常直接利用PD中每个点 $(b_i, d_i)$ 的坐标信息，即拓扑特征的出生时间 $b_i$、死亡时间 $d_i$ 以及持久性（或寿命）$p_i = d_i - b_i$。通过对这些数值进行统计汇总，可以从不同侧面反映数据的拓扑结构。
            
            一种基本的描述符是统计不同维度同调群（如$H_0, H_1, H_2$等）中检测到的拓扑特征的总数。这相当于计算PD中对应维度离对角线（即$d_i > b_i$）的点的数量（例如，参考文献  中进行了此类计算）。这个数量粗略地反映了数据在所考虑尺度范围内的拓扑复杂性。例如，在时间序列分析中，可以通过比较不同类别时间序列转化得到的点云所对应的PD中$H_1$（环状结构）特征的数量，来判断它们的结构差异（参考文献 ）。
            
            除了特征的数量，特征的“寿命”或持久性 $p_i = d_i - b_i$ 是一个更关键的指标，因为它反映了拓扑特征的显著性——寿命长的特征通常对应于数据中更稳定、更本质的结构，而寿命短（即靠近对角线的点）的特征则可能与噪声或细微结构有关。因此，一系列基于持久性的统计量被提出。例如，可以计算某个维度下所有特征的总寿命（$\sum p_i$），它反映了该维度拓扑特征的总体“能量”或显著性（参考文献 ）。将总寿命除以特征数量，则得到平均持久性（或平均寿命），它衡量了该维度特征的平均稳定性（参考文献 ）。研究发现，不同类别的数据在这些指标上可能呈现出显著差异，例如某一类时间序列可能普遍具有更长的平均$H_1$寿命（）。
            
            在所有特征中，具有最大持久性的特征通常被认为是最重要的拓扑不变量。因此，最大持久性（$\max p_i$）本身就是一个重要的统计描述符（参考文献 ）。它标识了数据中最“顽强”的拓扑结构。基于最大持久性，还可以定义“显著特征”的数量，即统计那些寿命超过某个阈值的特征个数，例如，可以取最大持久性的某个比例（如0.5倍）作为阈值（参考文献  中使用了这种方法）。这种方法有助于过滤掉可能由噪声产生的短寿命特征，更聚焦于数据的核心结构。
            
            另一种有价值的统计描述符是持续性熵（Persistence Entropy, PE）。它借鉴了信息论中香农熵的概念，基于PD中所有特征的寿命分布来计算。具体来说，将每个特征的寿命 $p_i$ 视为一个概率事件（通常通过归一化总寿命得到概率分布），然后计算其香农熵 $-\sum \frac{p_i}{L} \log(\frac{p_i}{L})$，其中 $L = \sum p_i$ 是总寿命（参考文献 ）。持续性熵提供了一个衡量PD中寿命分布复杂性或不确定性的单一数值（参考文献 ）。较高的熵值可能意味着寿命分布更均匀或更复杂。研究表明，持续性熵也可以作为区分不同数据类别的有效特征（参考文献 ），尽管也有观点认为其作为单一数值可能过于简化（参考文献 ）。
            
            此外，还可以考虑其他的统计量，例如寿命的方差、标准差、偏度或峰度，以更细致地刻画寿命分布的形态。或者，也可以考察拓扑特征之间的时间关系，例如计算在不同尺度下平均同时存在的同调群个数（参考文献 ），这间接反映了特征在时间（尺度）维度上的重叠程度和复杂性。
            
            总而言之，持久性图的统计描述符提供了一套相对简单且直观的方法，用于从PD中提取定量的拓扑信息。通过计算特征数量、寿命的总和、平均值、最大值、显著性计数以及熵等指标，可以从宏观层面捕捉数据的拓扑结构特征。这些描述符易于计算和解释，并且已被证明在某些应用中（如参考文献  中的时间序列分类）能够有效地揭示不同类别数据之间的结构差异。然而，需要认识到，这些单一或少量的统计数值必然会损失PD中包含的大量几何和结构细节。相比之下，后续章节将要介绍的基于贝蒂数的表示方法以及向量/函数空间嵌入方法（如PI, PL, PC等）则试图以更全面的方式保留PD的结构信息，尽管这通常伴随着更高的计算复杂度和解释难度。理解这些统计描述符的含义和局限性，是选择合适拓扑特征进行数据分析的第一步。
            
        \subsubsection{基于Betti数的表示}
            \label{sec:betti_features}
            % 原理：将Betti数视为滤过参数的函数或其离散采样。
            % 文献：引用 Umeda (2017) 的Betti序列及应用；提及 Chung et al. (2020) 将其视为持久性曲线特例。
            % 优缺点：反映特征数量变化，但损失位置信息。
            继上一节探讨了通过计算持久性图（PD）的各类统计量来概括其拓扑信息之后，本节将介绍另一类重要的特征表示方法，它们直接关注于拓扑不变量的核心——贝蒂数（Betti Numbers）及其在持续同调框架下的演化。贝蒂数是拓扑学中的基本概念，第 $k$ 阶贝蒂数 $\beta_k$ 直观地量化了一个拓扑空间中 $k$ 维“孔洞”的数量。具体而言，$\beta_0$ 代表空间中连通分支的数量，$\beta_1$ 代表独立环路（一维孔洞）的数量，$\beta_2$ 则代表空腔或空洞（二维孔洞）的数量，以此类推。在代数上，$\beta_k$ 定义为第 $k$ 阶同调群 $H_k(K)$ 的秩（rank）(参考文献 )。

            然而，在持续同调的语境下，我们分析的不是单个静态的拓扑空间，而是一个通过尺度参数（例如，在VR复形构造中的邻域半径 $\epsilon$）索引的嵌套复形序列，即“过滤”（Filtration），记作 $\{K_t\}_{t \ge 0}$，其中 $K_s \subseteq K_t$ 当 $s \le t$。因此，仅仅计算某个特定尺度 $t$ 下的瞬时贝蒂数 $\beta_k(K_t)$ 无法完全捕捉到拓扑特征随尺度变化的持续性信息。持续同调的核心正是追踪这些特征（孔洞）的“出生”和“死亡”过程。与此相关的是持续贝蒂数（Persistent Betti numbers）的概念，它统计了在尺度区间 $[i, j)$ 内持续存在的 $k$ 维特征的数量，这与持续同调群 $H_k^{i, j-i}$ 的秩有关 (概念参考)。
            
            尽管如此，考察瞬时贝蒂数 $\beta_k(K_t)$ 如何随尺度参数 $t$ 变化，本身就提供了一种有价值的拓扑描述方式。这种描述方式的核心是构建所谓的贝蒂曲线（Betti Curve）或贝蒂序列（Betti Sequence，当尺度参数离散时）。第 $k$ 阶贝蒂曲线是一个函数 $B_k: \mathbb{R}_{\ge 0} \rightarrow \mathbb{N}_0$，其在尺度 $t$ 处的值 $B_k(t)$ 就是复形 $K_t$ 的第 $k$ 阶贝蒂数，即 $B_k(t) = \beta_k(K_t)$ (相关概念见于 )。这条曲线描绘了 $k$ 维孔洞的数量在过滤过程中的动态变化情况。
            
            贝蒂曲线可以从持续同调的计算结果（如持久性条形码或持久性图）中导出。对于一个给定的尺度 $t$， $B_k(t)$ 的值等于在 $k$ 维持久性条形码中，那些生命周期区间 $[b_i, d_i)$ 包含 $t$（即满足 $b_i \le t < d_i$）的条带的数量。换言之，它统计了在尺度 $t$ 时“存活”的 $k$ 维拓扑特征的总数。
            
            从贝蒂曲线 $B_k(t)$ 的形态中可以解读出数据的拓扑演化信息。曲线的峰值对应于 $k$ 维孔洞数量达到极大的尺度范围，反映了该维度拓扑结构最为丰富的阶段。曲线的上升段表示新特征的诞生速度超过了旧特征的消亡速度，而下降段则反之。曲线积分 $\int B_k(t) dt$ 与对应维度下所有特征的总寿命（即上一节讨论的总持久性）密切相关。因此，贝蒂曲线的整体形状可以作为一种概括性的拓扑指纹。
            
            在实际应用中，贝蒂曲线本身（或其离散采样版本——贝蒂序列）可以直接用作特征向量。例如，可以将曲线在预定的一系列尺度值上进行采样，得到一个固定长度的向量，然后输入到机器学习分类器中。已有研究利用贝蒂序列作为特征成功应用于时间序列分类等任务 (例如 Umeda 的工作，参考文献)。相较于上一节的单一统计描述符，贝蒂曲线保留了拓扑特征数量随尺度变化的动态信息，提供了更丰富的描述。但相较于后面将要介绍的持续性图像或景观等方法，它仍然丢失了特征之间的配对信息（即哪个出生事件对应哪个死亡事件），仅仅追踪了总数的变化。这意味着不同的持久性图可能对应相同的贝蒂曲线。
            
            总结来说，基于贝蒂数的表示方法，特别是贝蒂曲线和序列，提供了一种连接经典拓扑不变量和持续同调的桥梁。它们通过关注拓扑特征数量随尺度的演化，为数据提供了一种中等粒度的拓扑描述符，兼具一定的解释性和信息量。理解贝蒂曲线的构建和解读，有助于我们从“计数”的角度把握数据的多尺度拓扑结构。接下来，我们将进一步探讨那些旨在更全面捕捉持久性图几何结构或提供不同类型数值总结的特征构造方法，如持续同调熵、持续性图像、持续性景观和持久性中心。
            
        \subsubsection{基于函数或映射的向量化方法}
            \label{sec:pd_vectorization_mapping}
            % 将PD映射为函数、图像或低维向量。

            % 注意：下一级的具体方法 PL, PI, PC, P-Center 维持为 \subsubsection 内部的普通段落或列表可能更好，
            % 避免层级过深。或者如果你确实需要区分，可以继续用 \subsubsection，但我这里暂时用 \paragraph 示意。
            前述的统计描述符和基于贝蒂数的表示为持久性图（PD）提供了有价值的定量总结，但它们通常以牺牲大量细节信息为代价。为了更全面地捕捉PD中蕴含的丰富几何与拓扑结构，并生成更适合现代机器学习技术的输入，研究者们开发了一系列基于特定函数或映射的向量化方法。这些方法的核心思想是将PD这个多重点集整体地、结构化地映射到一个向量空间（如 $\mathbb{R}^n$）或函数空间（如 $L^p$ 空间）中。其目标通常是生成稳定（即对输入数据的微小扰动不敏感）、信息丰富且计算上可行的特征表示。本节将分别介绍几种主流的此类方法：持续同调熵（PE）、持续性景观（PL）、持续性图像（PI）和持久性中心（PC）。

            \paragraph{持久性熵 (Persistence Entropy)}
            \label{sec:feat_pe}
            % 原理、文献、优缺点...
            在众多向量化方法中，持续同调熵（PE）可以被视为一种基于特定函数（信息熵函数）的极简总结方式。它并非生成高维向量，而是将持久性条形码或等价的PD信息压缩成一个单一的标量值。其计算依据是持久性条形码 $B = \{[b_i, d_i)\}_{i=1}^n$ 中每个条带的长度（寿命）$l_i = d_i - b_i$。借鉴香农熵的定义，持续同调熵被定义为 $PE(B) = -\sum_{i=1}^{n} \frac{l_i}{L} \log(\frac{l_i}{L})$，其中 $L = \sum_{i=1}^{n} l_i$ 是所有条带的总长度（参考文献 ）。这个值衡量了条带寿命分布的复杂性或不确定性。PE的优点在于其计算极其简单快速，得到的结果是一个易于使用的单一数值特征。然而，其最大的缺点是信息损失巨大，它完全忽略了特征的出生和死亡时间信息，仅考虑了寿命的分布，因此可能无法区分具有不同结构但寿命分布相似的PD。正如一些研究指出的，单一的PE数值在直接应用时可能过于片面（参考文献 ）。尽管如此，在某些特定场景下或作为辅助特征，PE仍有其应用价值（参考文献 ）。

            \paragraph{持久性景观 (Persistence Landscapes, PL)}
                \label{sec:feat_pl}
                % 原理、特点、文献、优缺点...
                持久性景观 (Persistence Landscapes) \cite{1} 是一种将持久性同调信息转化为函数空间元素的拓扑数据分析工具，因其向量空间特性而易于同统计学与机器学习方法相结合。其构建始于持久性模块 $M$，从中定义Betti数
                \begin{equation}
                \label{eq:betti_number} % 您可以为公式添加标签以便交叉引用
                \beta^{a,b} = \text{dim}(\text{im}(M(a \le b): M_a \rightarrow M_b))
                \end{equation}
                用以量化在尺度 $a$ 到 $b$ 间持续存在的拓扑特征数量。进一步，持久性景观被定义为一系列函数 $\lambda_k: \mathbb{R} \rightarrow \overline{\mathbb{R}}$，其中
                \begin{equation}
                \label{eq:lambda_k_definition}
                \lambda_k(t) = \text{sup}\{m \ge 0 \mid \beta^{t-m,t+m} \ge k\}
                \end{equation}
                这里 $\lambda_k(t)$ 直观表示在参数 $t$ 附近至少有 $k$ 个拓扑特征持续存在的最大“寿命半径” $m$。这些景观函数具有非负性 ($\lambda_k(t) \ge 0$)、对于固定的 $t$ 关于 $k$ 的单调递减性 ($\lambda_k(t) \ge \lambda_{k+1}(t)$) 以及1-Lipschitz连续性 ($|\lambda_k(t) - \lambda_k(s)| \le |t-s|$)。若给定一个由点集 $D = \{(b_i, d_i)\}_{i=1}^N$ 构成的持久性图，其中 $(b_i,d_i)$ 为第 $i$ 个特征的生灭时间对，则相应的景观函数 $\lambda_k(t)$ 可计算为所有
                \begin{equation}
                \label{eq:lambda_k_from_diagram_core}
                \max(0, \min(t-b_i, d_i-t))
                \end{equation}
                值中的第 $k$ 大值。这种将拓扑特征转化为函数的方法，使得对拓扑数据进行平均、定义范数和距离以及应用统计推断成为可能，为复杂数据分析提供了有力支持。
                


            \paragraph{持久性图像 (Persistence Images, PI)}
                \label{sec:feat_pi}
                % 原理、参数、优缺点、文献...
                持续性图像（PI）是当前应用最广泛的PD向量化方法之一，旨在生成稳定且易于被机器学习算法使用的固定维度向量表示 (参考文献 )，在多篇文献中均有提及或应用 (参考文献 )。PI的生成过程大致如下：首先，将PD中的点 $(b, d)$ 变换到“出生-持久性”坐标系，得到点 $(b, p)$，其中 $p = d-b$。接着，在每个变换后的点 $(b_i, p_i)$ 处放置一个二维概率密度函数（核函数），通常选用均值为 $(b_i, p_i)$、方差为 $\sigma^2$ 的高斯核 $\phi_{(b_i, p_i)}(x, y)$。为了强调持久性长的特征或根据需要调整不同区域的重要性，可以对每个核函数施加一个非负的权重函数 $f(b_i, p_i)$，该权重函数通常依赖于持久性 $p_i$。然后，将所有加权的核函数叠加起来，形成一个定义在 $\mathbb{R}^2$ 上的连续表面（或函数）$\rho_{B}(z) = \sum_i f(b_i, p_i) \phi_{(b_i, p_i)}(z)$（参考文献 ）。最后，在预先定义的二维网格上，对这个表面函数 $\rho_B$ 在每个像素（网格单元）$P$ 内进行积分 $\iint_P \rho_B(z) dz$，得到的积分值构成了该像素的灰度值。所有像素的值排列起来就形成了一个矩阵（图像）或一个向量（将矩阵展平）（参考文献 ）。PI的主要优势在于其表示的稳定性（参考文献 ）、生成结果的维度固定且属于欧氏空间，以及参数的灵活性（可以选择核函数、方差 $\sigma$、网格分辨率、权重函数等进行调整）。它也便于融合不同同调维度的信息，只需将各维度生成的PI向量拼接即可（参考文献 ）。但其缺点是结果可能受到参数选择的影响，并且如果PD中的点相对于网格分辨率来说过于稀疏，生成的PI向量可能包含大量零值（参考文献 ）。

            \paragraph{持久性曲线 (Persistence Curves, PC)}
                \label{sec:feat_pc}
                % 原理、文献、优缺点...
                持久性曲线（PC）是一种相对较新的PD向量化方法，旨在通过将PD映射到一个函数空间中来捕捉其几何结构 (参考文献 )。与PI类似，PC也使用核函数来平滑PD中的点，但它的核心思想是将PD视为一个函数而非单纯的点集。具体而言，PC通过对PD中的每个点 $(b_i, d_i)$ 施加一个高斯核函数 $\phi_{(b_i, d_i)}(x, y)$，并在整个平面上进行积分，得到一个连续的函数表示（参考文献 ）。这个函数可以被视为PD的“密度估计”，它在每个点 $(x, y)$ 处的值反映了PD中所有点对该位置的贡献。PC的优点在于它能够保留PD的几何信息，并且生成的结果是一个连续函数，可以直接用于后续分析或作为机器学习模型的输入。然而，PC的计算复杂度较高，并且对参数选择（如核函数类型、带宽等）敏感（参考文献 ）。
            \paragraph{持久性中心 (Persistence Centers)}
                \label{sec:feat_pcen}
                % 原理、文献、优缺点...
                与PL和PI试图捕捉PD整体分布不同，持久性中心（PC）提供了一种更为简洁的表示，它将PD总结为一个（或少数几个）代表性的点。一种具体的定义方式（参考文献 ）是计算PD中所有点 $(b_i, d_i)$ 的加权平均值，其中权重由持久性 $p_i = d_i - b_i$ 决定。具体地，持久性中心 $\Psi(P_D)$ 可以定义为向量 $\Psi(P_D) = \frac{\sum_{x=(b,d) \in P_D} (d-b) x}{\sum_{x=(b,d) \in P_D} (d-b)}$，其中 $x = (b, d) \in \mathbb{R}^2$。这个计算结果是一个二维向量，可以直观地理解为PD的“重心”，但这里的“质量”被赋予了每个点的持久性。这种表示方法的一个显著优点是其计算极其简单，并且通过持久性加权，它自然地强调了寿命长、结构上更显著的拓扑特征，同时降低了寿命短、可能源于噪声的特征的影响（参考文献）。然而，其代价是极高的信息压缩率，PC几乎丢失了PD中除加权平均位置之外的所有几何分布和结构细节。尽管如此，在某些应用场景下，例如作为后续分类器（如随机森林）的输入特征时，这种高度浓缩的表示可能仍然有效（参考文献 ）。

        \subsubsection{基于子窗口聚合的鲁棒PH特征提取}
        在将持续同调应用于较长时间序列的滑动窗口分析时，研究者常常面临两个主要的实际挑战：其一是，直接处理长窗口数据可能导致持久同调的计算复杂度过高，使得分析效率低下；其二是，长窗口内的时间序列数据更容易受到局部噪声或非典型波动的显著影响，这些干扰可能污染所提取的全局拓扑特征，降低其稳定性和判别能力。针对这些在实际应用中常见的问题，Karan和Kaygun \cite{3} 提出了一种创新性的基于子窗口化（Subwindowing）的拓扑特征提取策略。该策略旨在通过一种分而治之并结合统计聚合的方式，提高拓扑特征对噪声的鲁棒性，并为处理长窗口数据提供一种计算上更高效的途径。

        子窗口化方法的核心思想是，对于一个给定的、长度可能较长的主滑动窗口（Main Window）$W_N$，算法并不直接对整个 $W_N$ 进行全局的拓扑分析。取而代之的是，首先将主窗口 $W_N$ 分割为 $M$ 个长度更短、但可能相互重叠的子窗口（Subwindows），记为 $sw_1, sw_2, \dots, sw_M$ [36, 106]。随后，独立的拓扑特征提取过程将在每一个子窗口 $sw_i$ 上分别执行。值得注意的是，Karan和Kaygun在其研究中，为了从每个子窗口中获取更全面的拓扑信息，采用了多角度的分析手段：他们不仅对每个子窗口应用了基于不同嵌入维度（例如，与信号采样频率 $f_s$ 相关的多个特定比例，如 $0.5f_s, 1.0f_s, 1.5f_s, 2.0f_s$ [180]）的时间延迟嵌入并计算相应的持久性图，还额外计算了基于子窗口原始数值序列的上下水平集（upper and lower level set）过滤所产生的持久性图 [182]。通过这种方式，每一个子窗口都对应着一组（在他们的方法中是6个）不同的持久性图，从多个方面刻画了该子片段的拓扑结构。
        
        从每个子窗口的每一个持久性图中，可以进一步提取出一系列基础的拓扑特征描述符。Karan提及他们从每个持久性图的单个同调维度中提取了7种不同的特征，这些特征可能包括基于持久性图上点分布的统计量、基于Wasserstein或Bottleneck距离的度量、持久性熵，或是将持久性图转换为其他表示（如Betti曲线、持久性景观）后再计算其$L^1$或$L^2$范数等 。假设 $f_{i,k}$ 表示从第 $i$ 个子窗口的某个持久性图（或某个特定角度的分析）中提取的第 $k$ 种基础拓扑特征，那么主窗口 $W_N$ 的最终特征向量并不是直接由某个单一分析产生，而是通过统计聚合其内部所有 $M$ 个子窗口的这些基础特征而形成的。该研究中采用的聚合方式主要是计算每种基础特征 $f_{\cdot,k}$ 在所有子窗口上的均值（Mean）和标准差（Standard Deviation）。具体而言，主窗口 $W_N$ 关于第 $k$ 种特征的均值 $\mu_{N,k}$ 计算如下：
        \begin{equation}
            \mu_{N,k} = \frac{1}{M} \sum_{i=1}^{M} f_{i,k}
            \label{eq:subwindow_mean}
        \end{equation}
        而其标准差 $\sigma_{N,k}$ 则由下式给出：
        \begin{equation}
            \sigma_{N,k} = \sqrt{\frac{1}{M}\sum_{i=1}^{M}(f_{i,k} - \mu_{N,k})^2} = \sqrt{\left(\frac{1}{M}\sum_{i=1}^{M}f_{i,k}^2\right) - \mu_{N,k}^2}
            \label{eq:subwindow_std}
        \end{equation}
        通过这种聚合，主窗口的特征不仅概括了其内部拓扑特征的平均表现水平（由均值 $\mu_{N,k}$ 反映），还通过标准差 $\sigma_{N,k}$ 量化了这些拓扑特征在主窗口内部的变异程度或不稳定性 [37, 107, 189]，这本身也可能成为一项具有判别力的信息。
        
        Karan指出，他们提出的子窗口化策略具有若干显著的优点。首先，该方法能够有效增强拓扑特征对噪声的鲁棒性。现实中的时间序列数据常受到局部噪声或短暂异常波动的干扰，这些干扰若存在于某个子窗口内，其影响会被限制在该子窗口的特征提取结果中。当通过对所有 $M$ 个子窗口的特征进行平均（如公式 \ref{eq:subwindow_mean} 所示）等统计聚合操作时，这些局部的、随机的噪声影响有望被显著平滑或平均掉，从而使得主窗口的最终特征表示更为稳定，更能反映其整体的、持续的拓扑结构。
        
        其次，该策略为处理长主窗口时面临的计算效率问题提供了一种优化途径。持久同调的计算复杂度通常随数据点数的增加而快速增长。若直接对一个非常长的主窗口进行计算，可能会非常耗时。子窗口化将计算分解到多个更短的片段上。更为关键的是，Karan提出了一种高效的滚动更新机制来计算主窗口的聚合特征。当主窗口沿时间序列滑动一个步长时（假设其移出一个最旧的子窗口 $sw_1$，并移入一个新的子窗口 $sw_{M+1}$），新的主窗口 $W_{N+1}$ 的聚合特征（均值和标准差）可以直接根据旧的主窗口 $W_N$ 的聚合特征以及这两个发生变化的子窗口的特征进行递推更新，而无需对所有 $M$ 个子窗口的特征重新进行完整的求和计算 [113, 114, 313]。具体地，新的均值 $\mu_{N+1,k}$ 可以表示为：
        \begin{equation}
            \mu_{N+1,k} = \mu_{N,k} + \frac{f_{M+1,k} - f_{1,k}}{M}
            \label{eq:recursive_mean}
        \end{equation}
        同样地，新的方差 $\sigma_{N+1,k}^2$ (标准差的平方) 也可以从旧值递推得到：
        \begin{equation}
            \sigma_{N+1,k}^2 = \sigma_{N,k}^2 + (\mu_{N,k}^2 - \mu_{N+1,k}^2) + \frac{f_{M+1,k}^2 - f_{1,k}^2}{M}
            \label{eq:recursive_variance}
        \end{equation}
        这种递推关系使得在滑动窗口机制下，更新主窗口特征的计算成本在很大程度上与主窗口的实际长度 $M$ 解耦（在子窗口特征已知的前提下，更新的复杂度仅与少数几个变量相关），从而允许研究者在实践中使用较长的主窗口来捕捉更大范围的时间依赖性，而不会导致计算时间的急剧膨胀 [4, 121, 312, 313, 314]。
        
        此外，通过分析子窗口特征的统计分布（如标准差 $\sigma_{N,k}$），该方法还能间接捕捉到一个较长主窗口内部局部动态的变化情况。一个较大的标准差可能意味着主窗口内时间序列的拓扑特性正在经历显著变化，而较小的标准差则可能指示该段序列的拓扑结构相对稳定或均一。
        
        综上所述，Karan提出的基于子窗口聚合的拓扑特征提取策略，通过在更细粒度的子窗口上进行多角度拓扑分析，并结合统计聚合与高效的滚动更新机制，为时间序列的拓扑特征工程提供了一种实用且具有创新性的解决方案。它不仅致力于生成对噪声干扰更为鲁棒的特征表示，还通过巧妙的计算优化，有效应对了处理长段序列时可能面临的计算瓶颈，为基于TDA的时间序列分类及其他相关分析任务提供了有价值的方法论补充。
        
        \subsubsection{PH特征方法比较和讨论(关键)}
            \label{sec:tda_features_discussion}
            % 对比 TDA 特征提取方法的优劣...

    \subsection{时间序列的其他特征}
        \label{sec:other_ts_features}
        % 核心：介绍非TDA的时间序列特征提取和度量方法。
        在上文我们介绍了采用持续同调方法提取时间序列的拓扑特征,本小节旨在系统性的梳理和介绍拓扑数据分析之外的其他特征提取方法.拓扑特征能够捕捉时间序列数据的全局形状和连接性信息,但它们可能无法完全捕捉到数据的其他重要方面,例如局部统计特性,频率成分或特定的子序列模式。因此，探索和理解补充性的非拓扑特征对于构建更全面、更强大的TSC模型至关重要。这种对拓扑特征之外方法的探索需求本身就体现了一个重要的认知：没有任何单一类型的特征能够捕获时间序列中所有相关的信息。文献中将特征划分为统计特征、频域特征、时域特征（包括基于距离和形状的方法）、基于模型的特征以及深度学习自动提取的特征等多个类别 ，这进一步印证了采用多样化特征集来捕捉时间序列不同方面动态的必要性。
        \subsubsection{时域统计特征}
            \label{sec:ts_distances}
            % DTW, 编辑距离等...
            统计特征提取是最基础也是最常用的时间序列特征化方法之一。这类特征主要基于时间序列值的整体分布特性和基本的时间依赖关系进行描述。

            基本的描述性统计量概括了时间序列值的集中趋势、离散程度和分布形状。衡量数据中心位置的指标包括均值（Mean）、中位数（Median）和截尾均值（Trimmed Mean）。算术平均值可以代表物理系统的操作点或工作点，但对异常值非常敏感。在存在显著异常值的情况下，中位数和截尾均值是更稳健的选择。自动化库如 tsfresh 通常包含均值特征。衡量数据点围绕均值的离散程度或波动性的指标是方差（Variance）和标准差（Standard Deviation）。方差越大，表示数据波动越剧烈。tsfresh 也包含这些特征。

            分布形状的度量包括偏度（Skewness）和峰度（Kurtosis）。偏度衡量时间序列振幅概率密度函数（PDF）的对称性。偏度为零表示对称分布；正偏度（右偏）表示存在许多小值和少量大值；负偏度（左偏）表示存在许多大值和少量小值。偏度的变化可能指示相关物理系统的系统性变化。tsfresh 库计算偏度。峰度则衡量时间序列振幅PDF的“尖峰”程度。接近3的峰度值表示类似高斯分布的尖峰度。大于3表示相对尖锐的峰（尖峰），小于3表示相对平坦的峰（平峰）。峰度的变化同样可以反映系统的变化。tsfresh 库计算峰度。

            此外，最小值（Minimum）和最大值（Maximum）表示时间序列中的极端值，tsfresh 包含这些极值特征。分位数（Quantiles）指示数据分布中特定百分比位置的值，例如四分位数、百分位数等，tsfresh 也可以提取分位数特征。这些基本的统计特征可以通过 pandas、numpy 和 scipy.stats 等 Python 库方便地计算，而 tsfresh 等自动化库则能够自动计算其中的许多特征。

            自相关特征描述了时间序列与其自身在不同时间滞后（lag）版本之间的关系。自相关函数（Autocorrelation Function, ACF）测量时间序列与其滞后版本之间的相似度。ACF图常用于识别数据中的重复模式或季节性。例如，如果ACF在滞后12处出现显著峰值，可能表示存在年度季节性。tsfresh 可以计算聚合的自相关特征，例如不同滞后下的自相关系数的均值或方差。偏自相关函数（Partial Autocorrelation Function, PACF）测量序列与其滞后版本之间的相关性，同时控制（移除）了中间滞后的影响。PACF对于识别自回归（AR）模型的阶数特别有用。tsfresh 也提供了偏自相关系数的计算。

            时间序列的平稳性是一个核心概念，深刻影响着许多统计特征的有效性和解释性。一个平稳时间序列的统计特性（如均值、方差和自相关性）不随时间推移而改变。这意味着序列没有明显的趋势（长期增长或下降）或随时间变化的波动性。平稳性是许多经典时间序列模型（如ARIMA）和某些特征提取方法的基本假设。判断序列是否平稳通常结合视觉检查（观察时间序列图是否有明显趋势或变化的波动）和统计检验。常用的统计检验方法包括增广迪基-福勒检验（Augmented Dickey-Fuller, ADF Test），其原假设是时间序列存在单位根（即非平稳）。如果检验结果的p值小于显著性水平（如0.05），则拒绝原假设，认为序列是平稳的（或差分后平稳）。tsfresh 可以将ADF检验的统计量作为特征提取出来。另一常用检验是KPSS检验（Kwiatkowski-Phillips-Schmidt-Shin Test），其原假设是时间序列是（趋势）平稳的。

            如果时间序列被识别为非平稳，通常需要进行转换以使其平稳，然后再进行特征提取或模型拟合。常用的转换方法包括差分（Differencing），即计算连续观测值之间的差值（例如，$Y'_t = Y_t - Y_{t-1}$）。一阶差分可以消除线性趋势，二阶差分可以消除二次趋势。差分的阶数（d）是ARIMA模型中的一个重要参数。对数变换（Log Transformation）常用于稳定方差，特别是当数据随时间增长且波动性也随之增大时。去趋势（Detrending）可以通过拟合一个趋势模型（如线性回归）并从原始序列中减去该趋势，或者使用滤波器（如Hodrick-Prescott滤波器）移除趋势或周期成分来实现。均值、方差、ACF等基本统计特征的有效性和解释性在很大程度上取决于时间序列的平稳性。直接对具有明显趋势或变化的方差的非平稳序列计算单一的均值或方差，可能无法准确反映序列在不同时间点的行为，从而产生误导性的结果。因此，在提取这些统计特征之前，进行平稳性检验并根据需要应用差分等转换是一个必要的预处理步骤。

自动化库 tsfresh 提供了大量预定义的统计特征，除了上述提及的，还包括例如：绝对能量（Absolute Energy），即时间序列值的平方和 $\sum_{i=1}^{n} x_i^2$；绝对变化均值（Mean Absolute Change），即连续值之间绝对差值的平均值 $\frac{1}{n-1}\sum_{i=1}^{n-1} \lvert x_{i+1} - x_i \rvert$；峰值数量（Number of Peaks），计算时间序列中局部最大值的数量，可以指定峰值所需的最小支持邻域。此外，tsfresh 还包含多种熵度量（Entropy Measures），如基于值分箱计算熵的分箱熵（Binned Entropy），基于功率谱密度的傅里叶熵（Fourier Entropy），基于相邻值排序模式复杂性的排列熵（Permutation Entropy），衡量规律性的样本熵（Sample Entropy），以及衡量可预测性和复杂性的近似熵（Approximate Entropy）。

虽然这些基本的统计特征是理解时间序列的基础，并且计算相对简单，但它们主要捕捉的是线性依赖关系（如ACF）和整体的分布属性。它们可能难以捕捉到数据中复杂的非线性动态、瞬态事件或基于形状的模式。例如，一个具有复杂周期性或混沌行为的序列，其均值和方差可能无法完全反映其动态特性。这就引出了对更高级特征提取方法的需求，例如频域分析、形状分析和深度学习方法，这些方法将在后续章节中讨论。

        \subsubsection{频域特征}
            \label{sec:ts_statistical_spectral}
            % 时域统计量，频域特征...
频域分析是将时间序列从时域转换到频域，以揭示其周期性成分和频谱特性的重要方法。这种转换有助于识别在时域中不明显的重复模式或振荡行为。

基于傅里叶变换（Fourier Transform, FT）的特征是一种经典方法。傅里叶变换是一种信号处理技术，它将一个信号分解为其组成的正弦和余弦波的总和，每个波对应一个特定的频率。其核心思想是任何（足够规则的）周期信号都可以表示为不同频率和振幅的正弦波的叠加。对于时间序列分析，通常使用离散傅里叶变换（Discrete Fourier Transform, DFT）或其快速算法——快速傅里叶变换（Fast Fourier Transform, FFT）。傅里叶变换在频域提供了高分辨率，但在时域没有分辨率，这意味着它可以精确地告诉我们信号中存在哪些频率，但不能告诉我们这些频率在时间上的具体位置。因此，它最适用于分析频率成分不随时间变化的平稳信号。

从傅里叶变换中可以提取多种特征。DFT/FFT 的输出是一系列复数形式的傅里叶系数（Fourier Coefficients），每个系数对应一个特定的频率。这些系数的振幅（Magnitude）表示该频率分量的强度，相位（Phase）表示该频率分量的起始位置。可以直接使用某些频率的系数（或其振幅/相位）作为特征。tsfresh 库可以提取特定系数的实部、虚部、绝对值或角度作为特征。功率谱密度（Power Spectral Density, PSD）描述了信号功率如何随频率分布，它通常通过周期图（Periodogram）来估计，即傅里叶变换系数模平方的一种度量。PSD可以揭示信号中的主导频率，tsfresh 提供了基于Welch方法估计的PSD特征。频谱熵（Spectral Entropy）是基于PSD计算的熵值，衡量频谱的平坦度或复杂度。低频谱熵表示信号能量集中在少数频率上（可预测性高），高频谱熵表示能量分布广泛（更接近白噪声，可预测性低）；tsfresh 计算傅里叶熵（一种基于分箱的频谱熵）。频谱矩（Spectral Moments），如频谱质心（平均频率）、频谱方差（频率分布宽度）、频谱偏度、频谱峰度等，描述了频谱的整体形状，tsfresh 通过 fft\_aggregated 函数提取这些特征。频谱中的主导频率或峰值（Dominant Frequencies/Peaks）通常对应于信号中最显著的周期性成分。近年来，有研究开始探索直接在傅里叶域中对时间序列进行建模，例如神经傅里叶建模（Neural Fourier Modelling, NFM），认为这是一种紧凑且强大的时间序列分析方法，适用于预测、异常检测和分类等任务。这种方法利用傅里叶变换将有限长度时间序列视为函数空间中的连续时间元素，并允许在频域内进行重采样等操作。

为了克服傅里叶变换在分析非平稳信号（即频率成分随时间变化的信号）时的局限性，发展了基于小波变换（Wavelet Transform, WT）的特征提取方法。与傅里叶变换使用无限长的正弦波作为基函数不同，小波变换使用具有有限持续时间和特定形状的“母小波”（Mother Wavelet）及其伸缩（Dilation）和平移（Translation）版本作为基函数。这使得小波变换能够同时提供时间和频率（或尺度）的信息，实现时频局部化分析，特别适合分析包含瞬态事件、频率随时间变化的非平稳信号。

小波变换主要有两种形式。连续小波变换（Continuous Wavelet Transform, CWT）通过连续地伸缩和平移母小波来分析信号，提供非常详细的时频表示。其结果通常可视化为时频图，称为尺度图（Scalogram），显示了不同时间和尺度（对应频率）上的小波系数强度；tsfresh 可以提取CWT系数。离散小波变换（Discrete Wavelet Transform, DWT）则通过一系列高通和低通滤波器（滤波器组）对信号进行分解，通常采用二进尺度和位置。在每个分解层级（level），DWT将信号分解为近似系数（Approximation Coefficients，低频成分，代表信号的平滑部分或趋势）和细节系数（Detail Coefficients，高频成分，代表信号的细节或快速变化）。这个过程可以递归地应用于近似系数，从而得到多层分解，每一层对应不同的频率范围；sktime 库提供了 DWTTransformer，默认使用Haar小波进行分解。

从小波变换中提取特征的方式也多种多样。DWT产生的近似系数和各层细节系数本身可以作为特征，因为它们捕获了信号在不同时间和频率尺度上的信息。然而，由于小波系数的数量可能仍然很大，通常的做法是从每个分解层级（或称为子带, sub-band）的系数中提取统计特征，如均值、方差、标准差、能量、熵等。例如，可以计算每个细节系数子带的方差，以量化信号在不同频率段的波动性。tsfresh 库也提供了计算小波系数统计量的功能。对于CWT，其生成的尺度图可以被视为二维图像，然后可以利用强大的图像分类技术（如卷积神经网络CNN）来提取特征并进行分类。

在使用小波变换时，需要选择合适的母小波（如Haar, Daubechies, Symlets, Morlet等）和分解层数。母小波的选择取决于其形状特性（如对称性、紧支撑性）与待分析信号特征的匹配程度。分解层数的选择则关系到所能分析的最低频率和时频分辨率的权衡。这些选择往往需要领域知识或通过实验进行调整。例如，sktime 的 DWTTransformer 默认使用最简单的Haar小波，但其他库（如PyWavelets）或更高级的应用可能需要选择更复杂的母小波。

频域分析提供了一种与时域分析互补的视角。傅里叶变换是分析平稳周期性信号的有力工具，而小波变换则为分析具有时变频率特性或瞬态事件的非平稳信号提供了强大的时频局部化能力。两者之间的关键区别在于对信号平稳性的要求以及时频分辨率的权衡。傅里叶变换提供精确的频率信息但牺牲了时间信息，适用于频率成分固定的信号；小波变换则在时间和频率分辨率之间取得平衡，更适合分析动态变化的信号。无论是傅里叶系数还是小波系数，其数量都可能非常庞大。因此，在实际应用中，通常不是直接使用所有原始系数作为特征，而是对这些系数进行进一步处理，如计算系数的统计摘要（频谱熵、小波系数能量、子带方差等），或者将整个变换结果（如尺度图）视为一个整体输入到另一个机器学习模型（特别是CNN）中进行更高层次的特征学习。这种处理方式引入了抽象层级，使得最终特征代表了更复杂的频谱或时频模式。虽然小波变换功能强大，但其应用也引入了选择母小波和分解层数的复杂性，这与傅里叶变换相对标准化的应用形成了对比。

        \subsubsection{时域模式和形状特征}
            \label{sec:ts_subsequence_pattern}
            % Shapelets, Bag-of-Words/Symbolic...
            时域特征提取直接从时间序列的原始数值序列中进行，重点关注数据点之间的时间顺序、局部模式、形状以及滞后关系。

            利用时间序列的先前值来构建当前或未来值的预测特征是连接时间序列分析和监督学习的关键桥梁，这通常通过滞后与窗口特征实现。滞后特征（Lag Features）是最基本的思想，即使用过去某个时间点（如 $t-1, t-2, \dots$）的值 $x_{t-1}, x_{t-2}, \dots$ 作为预测当前或未来时间点（如 $t$ 或 $t+1$）值 $x_t$ 或 $x_{t+1}$ 的特征。这可以通过 pandas 库中的 \texttt{shift()} 函数轻松实现，该函数将序列向下移动指定的步数。通过包含多个滞后值作为输入特征，可以构建更复杂的模型。选择包含多少个滞后值（即窗口宽度）通常需要实验确定，并可能依赖于数据的自相关性。对于具有季节性的数据，还可以考虑包含更长周期的滞后值。
            
            滑动窗口特征（Rolling/Sliding Window Features）计算先前固定大小时间窗口内的值的摘要统计量，而不是直接使用滞后值。例如，可以使用过去5个时间点的平均值作为当前点的特征。常见的统计量包括均值、中位数、标准差、最小值、最大值等。这种方法有助于平滑噪声并捕捉局部趋势。pandas 的 \texttt{rolling()} 函数可以创建滑动窗口对象，然后在其上应用各种聚合函数（如 \texttt{.mean()}, \texttt{.std()}）来生成特征。扩展窗口特征（Expanding Window Features）与滑动窗口不同，它包含从序列开始到当前时间点的所有先前数据。基于扩展窗口计算的统计量（如累积均值、累积最大值）可以捕捉全局或累积性的信息，pandas 的 \texttt{expanding()} 函数用于此目的。在使用时，通常需要将序列 \texttt{shift(1)} 以确保用于生成特征的窗口不包含当前要预测的值。滞后和窗口特征是构建监督学习模型的基础，但它们主要捕捉的是基于固定时间间隔的依赖关系或局部统计汇总，对于需要识别特定形状模式或处理时间轴扭曲的情况，则需要更专门的方法。
            
            基于Shapelet的特征是另一种重要的时域方法。Shapelet方法的核心思想是在时间序列中寻找能够最好地区分不同类别的代表性子序列（即Shapelet）。分类决策基于一个时间序列与这些已发现的Shapelet的相似程度（通常用距离度量）。Shapelet是时间序列中的一个连续子序列，它对预测目标变量（类别标签）具有最大的区分能力。例如，在心电图（ECG）分类中，某个特定形状的波形段可能只出现在患有某种心脏病的患者的ECG序列中，这个波形段就可以作为一个Shapelet。Shapelet方法的一个主要优点是其可解释性：找到的Shapelet本身就是原始数据的一部分，可以直观地理解其代表的模式，为领域专家提供洞察。
            
            Shapelet变换（Shapelet Transform）是一种常用的利用Shapelet进行分类的方法。其过程包括：首先，从训练数据中搜索并选择出 $k$ 个最具区分能力的Shapelet，这通常涉及评估大量候选子序列区分不同类别的能力（例如，通过信息增益或F统计量）；其次，对于数据集中的每个时间序列，计算它到这 $k$ 个选定Shapelet的最小距离（通常定义为Shapelet与该时间序列中所有等长子序列之间距离的最小值），从而将每个原始时间序列转换成一个 $k$ 维的特征向量；最后，使用这个转换后的特征向量和原始标签来训练任何标准的分类器（如支持向量机SVM、随机森林、旋转森林等）。Shapelet算法存在多种变种，包括将Shapelet发现嵌入决策树构建的方法、通过优化算法同时学习Shapelet形状和分类器权重的学习Shapelet（Learning Shapelets, LS）、为提高效率随机采样候选Shapelet的方法（常与集成方法结合，如Random Shapelet Forest），以及结合卷积思想允许Shapelet跳点匹配以捕捉不同时间尺度模式的扩张Shapelet（Dilated Shapelets, 如DST, Castor）。Shapelet与时间序列之间的距离通常使用z标准化的欧氏距离（z-normalized Euclidean distance）计算，以消除基线漂移和振幅缩放的影响，但也可以使用动态时间规整（DTW）作为距离度量（如LTSD方法）来处理时间轴扭曲。sktime 和 pyts 等库提供了Shapelet变换的实现。
            
            动态时间规整（Dynamic Time Warping, DTW）本身也是一种强大的时域分析工具，主要用于测量两个可能在时间轴上发生扭曲或变速的时间序列之间的相似性。DTW通过寻找两条时间序列之间的最优非线性对齐路径（warping path）来计算它们之间的距离。它允许一个点与另一个序列中的多个点对齐，从而有效地处理时间序列之间的局部拉伸、压缩或相位移动，这是欧氏距离无法做到的。DTW距离是沿着这条最优路径累积的对应点之间的距离（通常是平方距离）的总和（或其平方根），通常使用动态规划算法找到最优路径。
            
            DTW最直接的应用是作为时间序列的距离度量，与k-近邻（k-NN）分类器结合使用，特别是1-NN DTW，曾被认为是时间序列分类（TSC）领域的基准方法。此外，DTW距离本身也可以用于特征提取。一种创新的方法是计算一个待分类时间序列与训练集中每一个时间序列的DTW距离，这些距离构成一个特征向量，输入到标准分类器中（如SVM）。这种方法利用了DTW捕捉相似性的能力，并将其与更强大的分类模型结合。同时，如前所述，DTW也可以替代欧氏距离，在Shapelet算法中计算Shapelet与时间序列的距离。DTW的主要缺点是其较高的计算复杂度，对于长度为 $n$ 和 $m$ 的序列，复杂度为 $O(nm)$（等长序列为 $O(n^2)$），远高于欧氏距离的 $O(n)$。这使得DTW在处理大规模数据集或长序列时效率低下。为缓解此问题，常使用约束（如Sakoe-Chiba带或Itakura平行四边形）限制搜索范围，或使用近似算法（如FastDTW、PrunedDTW），尽管可能牺牲最优性。多个Python库如 dtaidistance 和 tslearn 实现了DTW及其变种，sktime 也在其距离模块和分类器中集成了DTW。
            
            时域特征提取方法提供了多种捕捉时间序列动态的方式。滞后和窗口特征侧重于基于固定时间偏移或窗口的统计汇总和线性依赖关系。相比之下，Shapelet和DTW方法则侧重于模式匹配和形状相似性，能够处理相位独立性和时间扭曲。这种关注点的差异——统计聚合 vs. 模式/形状相似性——使得它们适用于不同类型的时间序列特性。在可解释性方面，Shapelet因其能够提供直观的、源自数据的模式而备受推崇。滞后和窗口特征也相对容易解释。然而，基于DTW距离的特征（即到所有训练样本的距离向量）维度可能很高，其解释性不如Shapelet直接，更像是基于样本在DTW距离空间中的相对位置进行分类。计算成本方面存在明显的层级：滞后/窗口特征通常最快（线性扫描）；DTW计算成本高（二次或约束下的超线性）；Shapelet发现也可能非常耗时，尽管随机化和变换方法等优化措施旨在降低这一成本。
        
        \subsubsection{基于模型的特征}
        与前述直接描述观测值统计特性或进行信号变换的方法不同，基于模型的特征提取采取了一种更为间接的策略。其核心思想是假设时间序列数据是由某个潜在的随机过程（stochastic process）生成的，并通过为每条时间序列拟合一个具体的统计模型来估计该过程的结构或参数。这些从拟合模型中得到的参数或导出量随后被用作该时间序列的特征表示。这种方法的吸引力在于，如果模型能够恰当地捕捉数据的生成机制，那么提取出的特征可能比直接的描述性统计量更能反映时间序列的内在动态和本质属性。本节将探讨几种经典的时间序列模型——ARIMA、GARCH和HMM——如何被用于特征提取，并讨论这种方法的优势与挑战。

首先，自回归积分移动平均（Autoregressive Integrated Moving Average, ARIMA）模型是时间序列分析中用于建模和预测具有线性时间依赖结构（或经过差分后呈现线性结构）的基石方法之一。ARIMA模型由三个阶数 $(p, d, q)$ 定义，其中 $p$ 代表自回归（AR）部分的阶数，反映了当前值对过去 $p$ 个自身值的线性依赖；$d$ 代表差分（I）的阶数，表示为使序列达到平稳状态所需的差分次数；$q$ 代表移动平均（MA）部分的阶数，反映了当前值对过去 $q$ 个预测误差项的依赖。ARIMA模型通过捕捉序列的自相关性和记忆性，为提取代表这种线性动态的特征提供了基础。从拟合的ARIMA模型中提取特征主要有两种方式：一是为每条时间序列确定的最优模型阶数组合 $(p, d, q)$ 本身就可以构成一组离散特征；二是拟合模型后得到的参数估计值，即自回归系数 $(\phi_1, \dots, \phi_p)$ 和移动平均系数 $(\theta_1, \dots, \theta_q)$，可以构成一组连续特征，它们直接量化了时间依赖性的强度和方向。Python中的 \texttt{statsmodels} 或 \texttt{sktime} 库可用于拟合ARIMA模型并获取阶数与系数，而 \texttt{tsfresh} 等库也能直接提取AR模型的系数。

在ARIMA模型主要关注序列均值的线性动态的同时，另一类重要的模型，广义自回归条件异方差（Generalized Autoregressive Conditional Heteroskedasticity, GARCH）模型，则专注于描述和预测许多时间序列（尤其是金融时间序列）中常见的时变波动性（即方差并非恒定，而是随时间变化并常常呈现“波动聚集”现象）。GARCH模型假设序列当前的条件方差 $\sigma_t^2$ 不仅依赖于过去的冲击（通常用误差平方项 $y_{t-1}^2$ 表示，即ARCH效应），还依赖于过去的条件方差 $\sigma_{t-1}^2$ 本身。一个常见的GARCH(1,1)模型可以表示为 $\sigma_t^2 = \alpha_0 + \alpha_1 y_{t-1}^2 + \beta_1 \sigma_{t-1}^2$。从GARCH模型中提取的特征主要包括拟合得到的模型参数估计值，如 $\hat{\alpha}_0, \hat{\alpha}_1, \hat{\beta}_1$ 等，这些参数直接刻画了时间序列波动性动态的关键特性（如基准波动水平、对冲击的敏感度、波动持续性）。此外，也可以计算由模型估计出的条件方差序列 $\sigma_t^2$ 的某些统计特性（如均值、最大值），或者模型的长期（无条件）方差预测值作为特征。实现GARCH模型拟合的常用工具包括Python的 \texttt{arch} 库和R语言的 \texttt{fGarch} 包。

与关注连续值动态的ARIMA和GARCH不同，隐马尔可夫模型（Hidden Markov Model, HMM）旨在对那些被认为是由一个潜在的、不可观测的离散状态序列驱动的观测序列进行建模。HMM假设系统在有限个隐藏状态之间随机转换（遵循马尔可夫性质），并且在每一个隐藏状态下，系统会根据特定的概率分布生成一个观测值。因此，HMM特别适用于建模时间序列中可能存在的“政权转换”（regime switching）或潜在模式变化。从HMM中提取特征可以从多个层面进行：一是通过训练算法（如Baum-Welch）学习到的模型参数本身（状态转移概率矩阵 $A$，观测发射概率 $B$，初始状态概率 $\pi$）；二是对给定的观测序列，使用Viterbi算法推断出最可能对应的隐藏状态序列，并从中提取特征（如状态频率、持续时间、转换次数）；三是在分类任务中为每个类别训练专属HMM，并使用新序列在各模型下的生成似然度作为特征或分类依据。Python的 \texttt{hmmlearn} 库是实现HMM的常用选择。

综上所述，基于模型的特征提取通过ARIMA、GARCH、HMM等工具，分别聚焦于时间序列的线性依赖结构、时变波动性以及潜在的离散状态转换机制。这提供了一个与描述性统计或变换域特征显著不同的视角，旨在通过模型参数揭示数据生成的内在规律。其优势在于可能获得对过程更具解释性的特征（如ARIMA的记忆性、GARCH的波动持续性、HMM的状态转移概率）。然而，这种方法的有效性高度依赖于模型假设与实际数据生成过程的匹配程度；模型选择不当可能导致特征失去意义。此外，为数据集中每条序列单独拟合模型通常计算成本较高，尤其对于复杂模型和大规模数据集。因此，在选择基于模型的特征时，需要在模型对特定数据动态的捕捉能力、计算可行性以及模型假设的合理性之间进行权衡。这些模型参数特征也为时间序列分类提供了另一类可能与拓扑特征、频域特征等互补的信息源。

            
        \subsubsection{基于深度学习的提取}
            \label{sec:ts_signature}
            近年来，深度学习（Deep Learning, DL）在时间序列分析领域取得了显著进展，尤其是在特征提取方面。与传统方法需要手动设计或选择特征不同，深度学习模型能够以端到端（end-to-end）的方式，直接从原始时间序列数据中自动学习层次化的、与任务相关的特征表示。

循环神经网络（Recurrent Neural Networks, RNN）及其变体是为处理序列数据而设计的神经网络结构。RNN按时间步顺序处理输入，并通过内部的隐藏状态（hidden state）来维持关于过去序列信息的一个“记忆”。这种循环连接使得RNN能够捕捉时间依赖性。基本的RNN结构（Vanilla RNN）在实践中容易遇到梯度消失或爆炸问题，难以学习长期依赖关系。为了克服这一局限性，长短期记忆网络（Long Short-Term Memory, LSTM）引入了门控机制（输入门、遗忘门、输出门）和记忆单元（cell state），能够选择性地记忆或遗忘信息，从而更好地捕捉长期依赖关系。门控循环单元（Gated Recurrent Unit, GRU）是LSTM的一种简化变体，使用更新门和重置门，参数更少，有时训练更快，性能与LSTM相当。在处理序列时，RNN（特别是LSTM和GRU）的隐藏状态或输出可以被视为在每个时间步学习到的特征表示。这些隐藏状态编码了到当前时间步为止的序列信息。可以将最后一个时间步的隐藏状态，或者所有时间步隐藏状态的某种聚合（如平均池化或最大池化）作为整个时间序列的特征向量，用于后续的分类任务。RNN提取的特征擅长捕捉序列的时间动态和上下文依赖，也常被用作混合模型的一部分，例如先用FFT处理信号再输入LSTM，或者与CNN结合。

卷积神经网络（Convolutional Neural Networks, CNN）最初在图像处理领域取得巨大成功，现已被广泛应用于时间序列分析。CNN的核心是卷积层，它使用小的滤波器（卷积核）在输入数据上滑动，以检测局部模式或特征。参数共享（同一滤波器应用于输入的不同位置）和池化层（降采样，减少维度并增加鲁棒性）是CNN的关键特性，使其在计算上相对高效且能够学习层次化特征。对于时间序列，常用的是一维CNN（1D CNN），它直接将卷积核应用于时间序列的一维数据上，滤波器沿着时间轴滑动。这使得1D CNN能够有效地捕捉时间序列中的局部时间模式或形状基元（motifs），例如特定形状的峰值或谷值。相比RNN，CNN通常可以并行计算，训练速度可能更快。另一种应用方式是二维CNN（2D CNN），这需要先将一维时间序列转换成二维的“图像”表示，然后应用标准的2D CNN架构。常见的转换方法包括将时间序列通过短时傅里叶变换（STFT）或连续小波变换（CWT）转换到时频域生成谱图（Spectrograms）或尺度图（Scalograms）；或者使用格拉姆角场（Gramian Angular Fields, GAF）、马尔可夫转移场（Markov Transition Fields, MTF）或递归图（Recurrence Plots）等方法将序列编码为图像。

CNN通过堆叠卷积层和池化层来学习层次化的特征。浅层卷积层通常学习简单的局部模式，而深层则组合这些低级特征来识别更复杂、更抽象的模式或形状。卷积层或池化层的输出（称为特征图, feature maps）的激活值可以被视为提取出的特征。在分类任务中，这些特征图通常会被展平并通过全连接层，或者使用全局平均池化（Global Average Pooling, GAP）来生成最终的分类预测。全卷积网络（FCN）、残差网络（ResNet）、InceptionTime等基于CNN的架构在时间序列分类（TSC）基准测试中表现出色。ROCKET及其变种（MultiRocket, Hydra）使用大量随机生成的卷积核进行特征提取，取得了非常好的速度和精度平衡。

Transformer架构最初为自然语言处理设计，现已成功应用于时间序列领域。Transformer的核心是自注意力机制（Self-Attention），它允许模型在处理序列中的每个元素时，动态地权衡序列中所有其他元素的重要性。这使得Transformer能够非常有效地捕捉长距离依赖关系和复杂的上下文信息，克服了RNN在处理非常长序列时的困难。Transformer已被用于时间序列预测、分类和异常检测等任务。由于时间序列数据与自然语言在结构上的差异，通常需要对原始Transformer架构进行调整，例如采用特定的嵌入（Embedding）方法将时间序列段（Patch）转换为输入令牌，或者修改注意力机制以更好地适应时间序列特性（如考虑局部性或周期性）。Transformer模型通常能处理变长序列。其编码器（或解码器）的中间层输出或最终的嵌入表示可以被视为学习到的特征，这些特征是上下文感知的，因为自注意力机制考虑了整个序列的信息来生成每个位置的表示。

自编码器（Autoencoder, AE）是一种无监督学习的神经网络，常用于降维和特征学习。AE由一个编码器（Encoder）和一个解码器（Decoder）组成。编码器将输入数据（如时间序列）映射到一个通常维度较低的潜在空间（latent space）表示（也称为编码或瓶颈层, bottleneck），解码器则尝试从这个潜在表示中重建原始输入。网络通过最小化重建误差（输入与输出之间的差异）进行训练。训练完成后，编码器部分可以将原始时间序列转换为其在潜在空间中的压缩表示。这个低维的潜在向量捕获了原始数据的主要变化因素或本质特征，可以作为后续监督学习任务（如分类）的输入特征。AE特别适用于高维时间序列的降维。变分自编码器（Variational Autoencoder, VAE）等变体还可以学习数据的概率分布。

深度学习方法代表了从基于领域知识或预定义变换（统计、频域、Shapelet）手动构建特征，转向隐式地在端到端模型训练过程中学习特征的范式转变。这些学习到的特征是针对特定任务（如分类）进行优化的。学习到的特征类型很大程度上取决于所使用的网络架构：CNN擅长学习局部的、层次化的时空模式；RNN捕捉序列依赖性；Transformer通过自注意力捕捉长距离上下文关系；自编码器学习压缩表示。架构的选择隐含地定义了特征学习的归纳偏置。尽管深度学习模型功能强大，但其学习到的特征（即网络中间层的激活值）通常比手动设计的特征（如均值、频谱功率或Shapelet）更难解释，常被称为“黑箱”模型。通常需要借助可视化技术或专门的可解释性方法（如LIME、SHAP）来理解模型到底学到了什么模式，这反映了自动化学习能力与人类理解之间的权衡。

\subsection{拓扑特征与其他特征的比较和讨论}
不同类型的特征各自侧重于时间序列的不同方面信息，具有独特的优势，同时也伴随着相应的局限性。为了在实际的时间序列分类任务中做出明智的特征选择决策，充分利用各种特征的潜力，对这两大类特征进行一次细致的比较与讨论显得尤为必要和关键。因此，本节旨在从信息内容、对噪声与变形的鲁棒性、特定不变性、结果的可解释性、计算代价以及适用场景等多个维度出发，系统地对比分析拓扑特征（源于4.1节）与其他主要时间序列特征（源于4.2节）。通过这种比较，期望能揭示各类特征的相对价值、潜在的协同作用，并为后续章节（特别是第五章）探讨特征在分类模型中的具体应用与集成策略奠定坚实的理论基础。
为了更清晰地理解基于持续同调的拓扑特征与第四章4.2节中讨论的各类传统及现代时间序列特征的异同，下表从多个维度对它们进行了比较：

\begin{table}[htbp] % h: here, t: top, b: bottom, p: page of its own
    \centering
    \caption{拓扑特征与其他时间序列特征的比较}
    \label{tab:feature_comparison}
    \small % Use smaller font size if needed
    \renewcommand{\arraystretch}{1.3} % Increase row spacing slightly for readability
    \begin{tabular}{>{\raggedright}p{0.18\textwidth} >{\raggedright}p{0.25\textwidth} >{\raggedright}p{0.25\textwidth} >{\raggedright\arraybackslash}p{0.25\textwidth}}
        \toprule
        \textbf{比较维度} & \textbf{拓扑特征 (TDA Features)} & \textbf{统计/频域特征} & \textbf{时域模式/形状/模型/深度特征} \\
        \midrule
        \textbf{核心关注点} &
        数据的全局形状、连通性、孔洞结构、多尺度拓扑不变量 。 &
        序列值的整体分布特性（均值、方差等 ）、周期性成分、频谱能量分布 。 &
        局部子序列模式 (Shapelets )、时间扭曲下的相似性 (DTW )、序列生成机制参数 (ARIMA/GARCH/HMM )、自动学习的层次化表示 (DL )。 \\
        \addlinespace
        \textbf{信息类型} &
        定性的几何/拓扑结构信息，通常对度量细节不敏感 。 &
        定量的统计量或频谱值，直接反映数值大小或频率强度 。 &
        定性（模式存在与否）或定量（距离、参数、激活值），捕捉局部/全局模式、动态或抽象表示 。 \\
        \addlinespace
        \textbf{鲁棒性} &
        对噪声、尺度变换、某些非线性变形具有较好的理论鲁棒性 。对离群点相对不敏感。 &
        基础统计量对离群点敏感 ，频域特征对噪声敏感 。平稳性是许多统计特征有效的前提 。 &
        Shapelet/DTW对噪声和变形有一定鲁棒性。模型特征依赖模型假设 。DL特征鲁棒性取决于训练和架构 。 \\
        \addlinespace
        \textbf{不变性} &
        潜在地对拉伸、压缩、连续变形具有不变性（取决于表示方法）。通常与坐标系无关。 &
        对平移、尺度变换通常敏感（除非进行标准化）。 &
        DTW对时间扭曲不变 。Shapelet经标准化后对幅值/偏移有不变性。其他特征不变性各异。 \\
        \addlinespace
        \textbf{可解释性} &
        中等。PD/条码图可直观展示特征生命周期 ，但向量化后（如PI/PL）解释性下降。 &
        高。统计量（均值、方差 ）、主频率 等物理意义明确。 &
        Shapelet解释性强。模型参数有物理解释 。DL特征通常是“黑箱”，解释性差。 \\
        \addlinespace
        \textbf{计算成本} &
        可能较高，尤其是PH计算和某些向量化方法（如PL。VR复形构建复杂度也较高。 &
        统计特征通常计算快 。FFT快，但WT可能稍慢。 &
        DTW计算成本高 ($O(n^2)$ [cite: 49])。Shapelet发现可能耗时 [cite: 49]。模型拟合  和DL训练  成本高。 \\
        \addlinespace
        \textbf{主要优点} &
        捕捉全局拓扑结构；对噪声/变形鲁棒；提供独特视角 。 &
        计算简单；物理意义清晰；理论成熟 。 &
        捕捉局部模式(Shapelet )；处理时间扭曲(DTW )；模型驱动解释(模型参数 )；自动学习复杂表示(DL )。 \\
        \addlinespace
        \textbf{主要缺点} &
        可能忽略幅值/频率等度量信息；计算成本高；向量化可能损失信息或引入参数 [cite: 39, 41]。 &
        对非线性/非平稳性处理能力弱；对噪声/离群点敏感；可能忽略全局结构 [cite: 43, 44]。 &
        计算成本高(DTW/Shapelet发现/模型/DL )；可解释性差(DL )；模型依赖假设 [cite: 52]；Shapelet/DTW可能忽略全局信息 。 \\
        \bottomrule
    \end{tabular}
    % 注意：表格后应有文字讨论，解释表格内容，强调特征互补性等。
\end{table}
 \subsection{本章小结}
        \label{sec:ch4_summary}
        % 总结本章介绍的特征提取方法...